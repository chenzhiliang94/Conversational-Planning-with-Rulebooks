name: Llama-3
llm_model:
  type: local
  model_config:
    pretrained_model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
    #device_map: 2
    load_in_8bit: True
    attn_implementation: "flash_attention_2"
  generation_config:
    max_new_tokens: 30

    # Beam search params
    num_beams: 5              # Number of beams to consider
    num_beam_groups: 5        # Number of beam groups (tries to encourage diversity between groups). Must be leq num_beams
    num_return_sequences: 5   # Number of samples to return. Must be leq num_beams
    diversity_penalty: 1.0    # Subtracted from a beam’s score if it generates a token same as any beam from other group at a particular time

    # Sample params
    do_sample: False          # Whether to use greedy decoding. Must be False for beam search
    temperature: 1.0
    top_p: 1.0
    repetition_penalty : 1.0  # 1.0 means no penalty.
  sys_prompt: "Imagine you are a human trying to converse with a language model. Please continue the following conversation by giving a random response. Keep your responses not too long. Try to ask clarifying questions if possible."
human_model:
  type: local
  model_config:
    pretrained_model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
    #device_map: 2
    load_in_8bit: True
    attn_implementation: "flash_attention_2"
  generation_config:
    max_new_tokens: 30

    # Beam search params
    num_beams: 5           # Number of beams to consider
    num_beam_groups: 5        # Number of beam groups (tries to encourage diversity between groups). Must be leq num_beams
    num_return_sequences: 5   # Number of samples to return. Must be leq num_beams
    diversity_penalty: 1.0    # Subtracted from a beam’s score if it generates a token same as any beam from other group at a particular time

    # Sample params
    do_sample: False          # Whether to use greedy decoding. Must be False for beam search
    temperature: 1.0
    top_p: 1.0
    repetition_penalty : 1.0  # 1.0 means no penalty
  sys_prompt: "You are an AI companion trying to converse with a human being. Please continue the following conversation by giving a random response. Keep your responses relatively short."


# Sample online model
# llm_model:
#   type: online
#   name: gpt-3.5-turbo-0125
#   device: 0
#   generation_config:
#     max_tokens: 30        # Maximum number of new tokens
#     n: 2                  # Number of chat completions
#     temperature: 1
#     frequency_penalty: 0  # Between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
#     presence_penalty: 0   # Between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
#   sys_prompt: "Imagine you are a human trying to converse with a language model. Please continue the following conversation by giving a random response. Keep your responses not too long."
# Sample local model

# human_model:
#   type: local
  # model_config:
  #   pretrained_model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
  #   device_map: 1
  #   load_in_8bit: True
  #   attn_implementation: "flash_attention_2"
#   generation_config:
#     max_new_tokens: 30

#     # Beam search params
#     num_beams: 3              # Number of beams to consider
#     num_beam_groups: 3        # Number of beam groups (tries to encourage diversity between groups). Must be leq num_beams
#     num_return_sequences: 3   # Number of samples to return. Must be leq num_beams
#     diversity_penalty: 1.0    # Subtracted from a beam’s score if it generates a token same as any beam from other group at a particular time

#     # Sample params
#     do_sample: False          # Whether to use greedy decoding. Must be False for beam search
#     temperature: 1.0
#     top_p: 1.0
#     repetition_penalty : 1.0  # 1.0 means no penalty
#   sys_prompt: "You are an AI companion trying to converse with a human being. Please continue the following conversation by giving a random response. Keep your responses not too long."